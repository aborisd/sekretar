import Foundation
import CoreML
import NaturalLanguage

// MARK: - Local Embedder (–∏–∑ ai_calendar_production_plan.md Week 1-2)

/// On-device –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è semantic search
/// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CoreML –º–æ–¥–µ–ª—å (Universal Sentence Encoder –∏–ª–∏ –∞–Ω–∞–ª–æ–≥)
class LocalEmbedder {

    // MARK: - Singleton

    static let shared = LocalEmbedder()

    // MARK: - Properties

    private let maxLength = 512 // Token limit
    private let embeddingDimension = 768

    // TODO: Load real CoreML model in Week 1-2
    // private let model: MLModel?

    private init() {
        // –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å CoreML –º–æ–¥–µ–ª—å
        // TODO: –î–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ Apple/HuggingFace
        /*
        guard let modelURL = Bundle.main.url(forResource: "SentenceEncoder", withExtension: "mlmodelc"),
              let model = try? MLModel(contentsOf: modelURL) else {
            print("‚ö†Ô∏è [LocalEmbedder] CoreML model not found, using fallback")
            self.model = nil
            return
        }
        self.model = model
        */

        print("üì¶ [LocalEmbedder] Initialized")
    }

    // MARK: - Embedding Generation

    /// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embedding –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞
    func embed(_ text: String) async throws -> [Float] {
        // TODO: –í Week 1-2 –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é CoreML –º–æ–¥–µ–ª—å

        // –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π word-based –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        return generateSimpleEmbedding(text)

        /*
        // Real implementation (Week 1-2):
        guard let model = model else {
            // Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥
            return generateSimpleEmbedding(text)
        }

        // Tokenize
        let tokens = tokenize(text)

        // Prepare input
        let inputArray = try MLMultiArray(shape: [1, NSNumber(value: tokens.count)], dataType: .int32)
        for (i, token) in tokens.enumerated() {
            inputArray[i] = NSNumber(value: token)
        }

        // Run model
        let input = SentenceEncoderInput(tokens: inputArray)
        let output = try model.prediction(from: input)

        // Extract embedding
        guard let embedding = output.featureValue(for: "embedding")?.multiArrayValue else {
            throw EmbeddingError.invalidOutput
        }

        // Convert to Float array
        return (0..<embedding.count).map { Float(truncating: embedding[$0]) }
        */
    }

    // MARK: - Simple Embedding (Fallback)

    /// –ü—Ä–æ—Å—Ç–æ–π embedding –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF –∏ word vectors
    /// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –¥–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ CoreML –º–æ–¥–µ–ª–∏
    private func generateSimpleEmbedding(_ text: String) -> [Float] {
        var embedding = [Float](repeating: 0.0, count: embeddingDimension)

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º NLEmbedding –æ—Ç Apple (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π)
        if let sentenceEmbedding = NLEmbedding.sentenceEmbedding(for: .english) {
            // –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if let vector = sentenceEmbedding.vector(for: text) {
                // NLEmbedding –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä –º–µ–Ω—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                // –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ 768 dimensions —Å padding
                for (index, value) in vector.enumerated() {
                    if index < embeddingDimension {
                        embedding[index] = Float(value)
                    }
                }
            }
        } else {
            // Ultimate fallback: —Ö—ç—à-based –ø–æ–¥—Ö–æ–¥
            embedding = hashBasedEmbedding(text)
        }

        // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
        return normalizeVector(embedding)
    }

    /// Hash-based embedding –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback
    private func hashBasedEmbedding(_ text: String) -> [Float] {
        var embedding = [Float](repeating: 0.0, count: embeddingDimension)

        // –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        let words = text.lowercased().split(separator: " ")

        for word in words {
            let hash = abs(word.hashValue)
            let index = hash % embeddingDimension

            // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
            embedding[index] += 1.0
        }

        return embedding
    }

    /// –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞ (L2 norm)
    private func normalizeVector(_ vector: [Float]) -> [Float] {
        let magnitude = sqrt(vector.reduce(0) { $0 + $1 * $1 })

        guard magnitude > 0 else {
            return vector
        }

        return vector.map { $0 / magnitude }
    }

    // MARK: - Tokenization

    /// –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (–¥–ª—è –±—É–¥—É—â–µ–π CoreML –º–æ–¥–µ–ª–∏)
    private func tokenize(_ text: String) -> [Int32] {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º NLTokenizer –æ—Ç Apple
        let tokenizer = NLTokenizer(unit: .word)
        tokenizer.string = text

        var tokens: [Int32] = []

        tokenizer.enumerateTokens(in: text.startIndex..<text.endIndex) { range, _ in
            let word = String(text[range])
            let hash = abs(word.lowercased().hashValue)
            let tokenId = Int32(hash % 50000) // Vocabulary size = 50k
            tokens.append(tokenId)

            return tokens.count < maxLength
        }

        return tokens
    }

    // MARK: - Batch Embedding

    /// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    func embedBatch(_ texts: [String]) async throws -> [[Float]] {
        // TODO: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–º–æ—â—å—é batch inference –≤ CoreML

        var embeddings: [[Float]] = []

        for text in texts {
            let embedding = try await embed(text)
            embeddings.append(embedding)
        }

        return embeddings
    }

    // MARK: - Semantic Similarity

    /// –í—ã—á–∏—Å–ª—è–µ—Ç semantic similarity –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏
    func semanticSimilarity(_ text1: String, _ text2: String) async throws -> Float {
        let embedding1 = try await embed(text1)
        let embedding2 = try await embed(text2)

        return cosineSimilarity(embedding1, embedding2)
    }

    private func cosineSimilarity(_ a: [Float], _ b: [Float]) -> Float {
        guard a.count == b.count else { return 0 }

        var dotProduct: Float = 0
        var normA: Float = 0
        var normB: Float = 0

        for i in 0..<a.count {
            dotProduct += a[i] * b[i]
            normA += a[i] * a[i]
            normB += b[i] * b[i]
        }

        let denominator = sqrt(normA) * sqrt(normB)

        return denominator > 0 ? dotProduct / denominator : 0
    }
}

// MARK: - Errors

enum EmbeddingError: Error {
    case modelNotLoaded
    case invalidOutput
    case tokenizationFailed
}

// MARK: - Future CoreML Model Input/Output

/*
// –≠—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –∫–æ–≥–¥–∞ –¥–æ–±–∞–≤–∏–º —Ä–µ–∞–ª—å–Ω—É—é CoreML –º–æ–¥–µ–ª—å

struct SentenceEncoderInput: MLFeatureProvider {
    let tokens: MLMultiArray

    var featureNames: Set<String> {
        return ["tokens"]
    }

    func featureValue(for featureName: String) -> MLFeatureValue? {
        if featureName == "tokens" {
            return MLFeatureValue(multiArray: tokens)
        }
        return nil
    }
}

struct SentenceEncoderOutput: MLFeatureProvider {
    let embedding: MLMultiArray

    var featureNames: Set<String> {
        return ["embedding"]
    }

    func featureValue(for featureName: String) -> MLFeatureValue? {
        if featureName == "embedding" {
            return MLFeatureValue(multiArray: embedding)
        }
        return nil
    }
}
*/
