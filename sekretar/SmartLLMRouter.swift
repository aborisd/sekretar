import Foundation
import CryptoKit

// MARK: - Smart LLM Router (–∏–∑ ai_calendar_production_plan.md Week 3-4)

/// –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä LLM –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ costs (60-70% —ç–∫–æ–Ω–æ–º–∏—è)
/// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
@MainActor
class SmartLLMRouterService {

    // MARK: - Properties

    private let classifier: ComplexityClassifier
    private let cache: LLMCache
    private let analytics: RouterAnalytics

    // Callback –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∏–Ω–∂–µ–∫—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑–≤–Ω–µ)
    private var generateFunc: ((LLMModel, LLMRequest) async throws -> LLMResponse)?

    // MARK: - Initialization

    init() {
        self.classifier = ComplexityClassifier()
        self.cache = LLMCache()
        self.analytics = RouterAnalytics()

        print("üß† [SmartRouter] Initialized with intelligent routing")
    }

    /// –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    func setGenerateFunction(_ generate: @escaping (LLMModel, LLMRequest) async throws -> LLMResponse) {
        self.generateFunc = generate
        print("üîó [SmartRouter] Connected to real LLM provider")
    }

    // MARK: - Main Routing

    /// –†–æ—É—Ç–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π LLM —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    func route(_ request: LLMRequest) async throws -> LLMResponse {
        let startTime = Date()

        // 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º cache first
        let cacheKey = generateCacheKey(request)
        if let cached = await cache.get(cacheKey) {
            analytics.recordCacheHit(model: cached.modelUsed)
            print("üíæ [SmartRouter] Cache HIT for: \(request.input.prefix(50))...")

            return cached
        }

        print("üîç [SmartRouter] Cache MISS, routing to LLM...")

        // 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        let classification = classifier.classifyDetailed(request.input)
        print("  Complexity: \(classification.complexity.description)")
        print("  Confidence: \(String(format: "%.1f%%", classification.confidence * 100))")

        // 3. –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ + user tier
        let selectedModel = selectModel(
            complexity: classification.complexity,
            userTier: request.userTier
        )

        print("  Selected model: \(selectedModel)")

        // 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å retry logic
        let response: LLMResponse
        if let generate = generateFunc {
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            response = try await generateWithRetry(
                model: selectedModel,
                request: request,
                maxRetries: 2,
                generate: generate
            )
        } else {
            // Fallback –Ω–∞ mock –µ—Å–ª–∏ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ —Ä–µ–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            response = try await mockGenerate(model: selectedModel, request: request)
        }

        // 5. –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        let ttl = cacheTTL(for: classification.complexity)
        await cache.set(cacheKey, response, ttl: ttl)

        // 6. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
        let latency = Date().timeIntervalSince(startTime)
        analytics.recordRequest(
            complexity: classification.complexity,
            model: selectedModel,
            userTier: request.userTier,
            latencyMs: latency * 1000,
            cachedResponse: false,
            success: true
        )

        return response
    }

    // MARK: - Model Selection

    /// –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ user tier
    private func selectModel(
        complexity: Complexity,
        userTier: SubscriptionTier
    ) -> LLMModel {

        switch (complexity, userTier) {
        // Simple queries - –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º Gemini Flash (cheapest)
        case (.simple, _):
            return .geminiFlash

        // Medium queries
        case (.medium, .free), (.medium, .basic):
            return .geminiFlash  // –≠–∫–æ–Ω–æ–º–∏–º –¥–ª—è free/basic users
        case (.medium, .pro), (.medium, .premium), (.medium, .teams):
            return .gpt4oMini   // –õ—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –ø–ª–∞—Ç–Ω—ã—Ö

        // Complex queries
        case (.complex, .free), (.complex, .basic):
            return .gpt4oMini   // Fallback –¥–ª—è free users
        case (.complex, .pro), (.complex, .premium), (.complex, .teams):
            return .claudeSonnet  // Premium –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
        }
    }

    // MARK: - Generation with Retry

    /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å retry logic –∏ fallback
    private func generateWithRetry(
        model: LLMModel,
        request: LLMRequest,
        maxRetries: Int,
        generate: (LLMModel, LLMRequest) async throws -> LLMResponse
    ) async throws -> LLMResponse {

        var lastError: Error?

        for attempt in 0...maxRetries {
            do {
                let response = try await generate(model, request)

                if attempt > 0 {
                    print("  ‚úÖ Retry successful on attempt \(attempt + 1)")
                }

                return response

            } catch let error as RateLimitError {
                lastError = error
                print("  ‚ö†Ô∏è Rate limit hit, attempt \(attempt + 1)/\(maxRetries + 1)")

                if attempt < maxRetries {
                    // Exponential backoff
                    let delay = pow(2.0, Double(attempt))
                    try await Task.sleep(for: .seconds(delay))
                } else {
                    // Last attempt - try fallback model
                    print("  üîÑ Switching to fallback model...")
                    return try await generateWithFallback(model: model, request: request, generate: generate)
                }
            } catch {
                lastError = error
                throw error
            }
        }

        throw lastError ?? LLMError.maxRetriesExceeded
    }

    /// Fallback –Ω–∞ –±–æ–ª–µ–µ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
    private func generateWithFallback(
        model: LLMModel,
        request: LLMRequest,
        generate: (LLMModel, LLMRequest) async throws -> LLMResponse
    ) async throws -> LLMResponse {

        let fallbackModel: LLMModel = {
            switch model {
            case .claudeSonnet:
                return .gpt4oMini
            case .gpt4oMini:
                return .geminiFlash
            case .geminiFlash:
                return .geminiFlash // Can't fallback further
            }
        }()

        print("  üìâ Using fallback: \(fallbackModel)")
        return try await generate(fallbackModel, request)
    }

    // MARK: - Cache Management

    /// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è cache key –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    private func generateCacheKey(_ request: LLMRequest) -> String {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º SHA256 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ö—ç—à–∞
        let data = Data(request.input.utf8)
        let hash = SHA256.hash(data: data)
        return hash.compactMap { String(format: "%02x", $0) }.joined()
    }

    /// TTL –¥–ª—è –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    private func cacheTTL(for complexity: Complexity) -> TimeInterval {
        switch complexity {
        case .simple:
            return 3600    // 1 —á–∞—Å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        case .medium:
            return 1800    // 30 –º–∏–Ω—É—Ç –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö
        case .complex:
            return 600     // 10 –º–∏–Ω—É—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö
        }
    }

    // MARK: - Analytics

    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ—É—Ç–µ—Ä–∞
    func getStats() -> RouterStats {
        return analytics.getStats()
    }

    /// –°–±—Ä–æ—Å–∏—Ç—å –∫—ç—à
    func clearCache() async {
        await cache.clear()
        print("üóëÔ∏è [SmartRouter] Cache cleared")
    }

    // MARK: - Mock Generation (TODO: Remove –≤ Week 3-4)

    private func mockGenerate(
        model: LLMModel,
        request: LLMRequest
    ) async throws -> LLMResponse {
        // –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        let delay: TimeInterval = {
            switch model {
            case .geminiFlash: return 0.1
            case .gpt4oMini: return 0.3
            case .claudeSonnet: return 0.5
            }
        }()

        try await Task.sleep(for: .seconds(delay))

        // Mock response
        return LLMResponse(
            content: "Mock response from \(model.rawValue)",
            modelUsed: model,
            tokensUsed: TokenUsage(input: 100, output: 50),
            latencyMs: delay * 1000,
            cached: false
        )
    }
}

// MARK: - LLM Cache

/// Actor –¥–ª—è thread-safe –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
actor LLMCache {
    private var storage: [String: CachedResponse] = [:]
    private var accessCount: [String: Int] = [:] // –î–ª—è LRU

    struct CachedResponse {
        let response: LLMResponse
        let expiry: Date
    }

    func get(_ key: String) -> LLMResponse? {
        // –£–¥–∞–ª—è–µ–º expired
        if let cached = storage[key] {
            if cached.expiry > Date() {
                accessCount[key, default: 0] += 1
                return cached.response
            } else {
                storage.removeValue(forKey: key)
                accessCount.removeValue(forKey: key)
            }
        }
        return nil
    }

    func set(_ key: String, _ response: LLMResponse, ttl: TimeInterval) {
        let expiry = Date().addingTimeInterval(ttl)
        storage[key] = CachedResponse(response: response, expiry: expiry)
        accessCount[key] = 0

        // LRU eviction –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        if storage.count > 1000 {
            evictLRU()
        }
    }

    func clear() {
        storage.removeAll()
        accessCount.removeAll()
    }

    private func evictLRU() {
        // –£–¥–∞–ª—è–µ–º 20% –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö
        let toRemove = Int(Double(storage.count) * 0.2)
        let sorted = accessCount.sorted { $0.value < $1.value }

        for (key, _) in sorted.prefix(toRemove) {
            storage.removeValue(forKey: key)
            accessCount.removeValue(forKey: key)
        }
    }
}

// MARK: - Analytics

class RouterAnalytics {
    private var requestCount: [LLMModel: Int] = [:]
    private var cacheHits: [LLMModel: Int] = [:]
    private var totalLatency: [LLMModel: Double] = [:]
    private var costSavings: Double = 0

    func recordRequest(
        complexity: Complexity,
        model: LLMModel,
        userTier: SubscriptionTier,
        latencyMs: Double,
        cachedResponse: Bool,
        success: Bool
    ) {
        if cachedResponse {
            cacheHits[model, default: 0] += 1
        } else {
            requestCount[model, default: 0] += 1
            totalLatency[model, default: 0] += latencyMs
        }

        // –í—ã—á–∏—Å–ª—è–µ–º —ç–∫–æ–Ω–æ–º–∏—é –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ–π
        if model == .geminiFlash && complexity == .complex {
            let savedCost = complexity.estimatedCostPer1KTokens.input - model.costPer1KTokens.input
            costSavings += savedCost
        }
    }

    func recordCacheHit(model: LLMModel) {
        cacheHits[model, default: 0] += 1
    }

    func getStats() -> RouterStats {
        let totalRequests = requestCount.values.reduce(0, +)
        let totalCacheHits = cacheHits.values.reduce(0, +)
        let cacheHitRate = totalRequests > 0 ? Double(totalCacheHits) / Double(totalRequests + totalCacheHits) : 0

        return RouterStats(
            totalRequests: totalRequests,
            cacheHits: totalCacheHits,
            cacheHitRate: cacheHitRate,
            modelUsage: requestCount,
            averageLatency: calculateAverageLatency(),
            estimatedCostSavings: costSavings
        )
    }

    private func calculateAverageLatency() -> [LLMModel: Double] {
        var avg: [LLMModel: Double] = [:]
        for (model, total) in totalLatency {
            let count = requestCount[model] ?? 1
            avg[model] = total / Double(count)
        }
        return avg
    }
}

// MARK: - Data Models

enum LLMModel: String, Codable {
    case geminiFlash = "gemini-1.5-flash"
    case gpt4oMini = "gpt-4o-mini"
    case claudeSonnet = "claude-sonnet-4.5"

    var costPer1KTokens: (input: Double, output: Double) {
        switch self {
        case .geminiFlash:
            return (0.075, 0.30)
        case .gpt4oMini:
            return (0.15, 0.60)
        case .claudeSonnet:
            return (3.0, 15.0)
        }
    }
}

// Note: SubscriptionTier is now defined in AuthManager.swift

struct LLMRequest {
    let input: String
    let userTier: SubscriptionTier
    let context: [String: Any]

    init(input: String, userTier: SubscriptionTier = .free, context: [String: Any] = [:]) {
        self.input = input
        self.userTier = userTier
        self.context = context
    }
}

struct LLMResponse {
    let content: String
    let modelUsed: LLMModel
    let tokensUsed: TokenUsage
    let latencyMs: Double
    let cached: Bool
}

struct TokenUsage {
    let input: Int
    let output: Int

    var total: Int {
        return input + output
    }
}

struct RouterStats {
    let totalRequests: Int
    let cacheHits: Int
    let cacheHitRate: Double
    let modelUsage: [LLMModel: Int]
    let averageLatency: [LLMModel: Double]
    let estimatedCostSavings: Double

    var summary: String {
        return """
        üìä Smart Router Statistics
        Total Requests: \(totalRequests)
        Cache Hits: \(cacheHits) (\(String(format: "%.1f%%", cacheHitRate * 100)))
        Model Usage:
        \(modelUsage.map { "  - \($0.key.rawValue): \($0.value)" }.joined(separator: "\n"))
        Estimated Savings: $\(String(format: "%.2f", estimatedCostSavings))
        """
    }
}

// MARK: - Errors

struct RateLimitError: Error {
    let retryAfter: TimeInterval
}

enum LLMError: Error {
    case maxRetriesExceeded
    case allModelsFailed
    case invalidResponse
}
